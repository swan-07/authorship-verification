{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textcomplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "from features import get_transformer, merge_entries\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from utills import chunker\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transformers(data_dict, data_fraction=0.01):\n",
    "    docs_1 = []\n",
    "    docs_2 = []\n",
    "    #train dset\n",
    "    num_entries = len(data_dict['text1'])\n",
    "    \n",
    "    for i in tqdm(range(num_entries), desc=\"Reading dataset\"):\n",
    "        if np.random.rand() < data_fraction:\n",
    "            docs_1.append(data_dict['text1'][i])\n",
    "            docs_2.append(data_dict['text2'][i])\n",
    "           \n",
    "    transformer = get_transformer()\n",
    "    scaler = StandardScaler()\n",
    "    secondary_scaler = StandardScaler()\n",
    "\n",
    "    X = transformer.fit_transform(docs_1 + docs_2).todense()\n",
    "    X = np.asarray(X)\n",
    "    X = scaler.fit_transform(X)\n",
    "    X1 = X[:len(docs_1)]\n",
    "    X2 = X[len(docs_1):]\n",
    "    secondary_scaler.fit(np.abs(X1 - X2))\n",
    "    \n",
    "    return transformer, scaler, secondary_scaler, X.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vectorize(XX, Y, ordered_idxs, transformer, scaler, secondary_scaler, data_dict, vector_Sz):\n",
    "    batch_size = 10000\n",
    "    docs1 = []\n",
    "    docs2 = []\n",
    "    idxs = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "\n",
    "    num_entries = len(data_dict['text1'])\n",
    "    \n",
    "    for idx in tqdm(range(num_entries), total=vector_Sz, desc=\"Vectorizing dataset\"):\n",
    "        docs1.append(data_dict['text1'][idx])\n",
    "        docs2.append(data_dict['text2'][idx])\n",
    "        labels.append(data_dict['score'][idx])\n",
    "        idxs.append(ordered_idxs[i])\n",
    "        i += 1\n",
    "        \n",
    "        if len(labels) >= batch_size:\n",
    "            x1 = transformer.transform(docs1).todense()\n",
    "            x2 = transformer.transform(docs2).todense()\n",
    "            x1 = np.asarray(x1)\n",
    "            x2 = np.asarray(x2)\n",
    "            x1 = scaler.transform(x1)\n",
    "            x2 = scaler.transform(x2)\n",
    "            XX[idxs, :] = secondary_scaler.transform(np.abs(x1 - x2))\n",
    "            Y[idxs] = labels\n",
    "\n",
    "            docs1 = []\n",
    "            docs2 = []\n",
    "            idxs = []\n",
    "            labels = []\n",
    "\n",
    "        if len(labels) > 0:\n",
    "            x1 = transformer.transform(docs1).todense()\n",
    "            x2 = transformer.transform(docs2).todense()\n",
    "            x1 = np.asarray(x1)\n",
    "            x2 = np.asarray(x2)\n",
    "            x1 = scaler.transform(x1)\n",
    "            x2 = scaler.transform(x2)\n",
    "            XX[idxs, :] = secondary_scaler.transform(np.abs(x1 - x2))\n",
    "            Y[idxs] = labels\n",
    "            XX[idxs, :] = secondary_scaler.transform(np.abs(x1-x2))\n",
    "            Y[idxs] = labels\n",
    "        XX.flush()\n",
    "        Y.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the HfApi\n",
    "api = HfApi()\n",
    "file_path = hf_hub_download(repo_id=\"swan07/process_chunks\", filename=\"processed_eval.pkl\", repo_type=\"dataset\")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testdset.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('testdset.pkl', \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('dset.pkl', \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/workspace/testdsets.pickle', 'rb') as f:\n",
    "    loaded_test_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = train.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_test_datasets_sz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mloaded_test_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded_test_datasets Sz:\u001b[39m\u001b[38;5;124m'\u001b[39m, loaded_test_datasets_sz, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text1'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sz = len(train_dict['text1'])\n",
    "test_sz = len(test_dict['text1'])\n",
    "\n",
    "print('Train Sz:', train_sz, flush=True)\n",
    "print('Test Sz:', test_sz, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting transformer...', flush=True)\n",
    "transformer, scaler, secondary_scaler, feature_sz = fit_transformers(train_dict, data_fraction=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing train set...', flush=True)\n",
    "XX_train = np.memmap('vectorized_XX_train.npy', dtype='float32', mode='w+', shape=(train_sz, feature_sz))\n",
    "Y_train = np.memmap('Y_train.npy', dtype='int32', mode='w+', shape=(train_sz))\n",
    "train_idxs = np.array(range(train_sz))\n",
    "np.random.shuffle(train_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainaaa = [XX_train, \n",
    "    Y_train, \n",
    "    train_idxs, \n",
    "    transformer, \n",
    "    scaler, \n",
    "    secondary_scaler, \n",
    "    train_dict,\n",
    "    train_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainaaa.pkl', 'wb') as f:\n",
    "    pickle.dump(trainaaa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"trainaaa.pkl\",\n",
    "    path_in_repo=\"trainaaa.pkl\",\n",
    "    repo_id=\"swan07/process_chunks\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(\n",
    "    XX_train, \n",
    "    Y_train, \n",
    "    train_idxs, \n",
    "    transformer, \n",
    "    scaler, \n",
    "    secondary_scaler, \n",
    "    train_dict,\n",
    "    train_sz\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing test set...', flush=True)\n",
    "XX_test = np.memmap('vectorized_XX_test.npy', dtype='float32', mode='w+', shape=(test_sz, feature_sz))\n",
    "Y_test = np.memmap('Y_test.npy', dtype='int32', mode='w+', shape=(test_sz))\n",
    "test_idxs = np.array(range(test_sz))\n",
    "np.random.shuffle(test_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def process_batch_gpu(batch_data, transformer, scaler, secondary_scaler):\n",
    "    docs1, docs2, labels, idxs = batch_data\n",
    "\n",
    "    print(f\"Processing batch with {len(docs1)} documents\")\n",
    "\n",
    "    # Check for validity of the batch\n",
    "    for doc1, doc2 in zip(docs1, docs2):\n",
    "        if not doc1.get('tokens') or not doc2.get('tokens'):\n",
    "            print(\"Skipping batch due to empty tokens.\")\n",
    "            return [], [], []\n",
    "        if len(doc1['tokens']) != len(doc1.get('pos_tags', [])) or len(doc2['tokens']) != len(doc2.get('pos_tags', [])):\n",
    "            print(\"Skipping batch due to mismatched lengths of tokens and pos_tags.\")\n",
    "            return [], [], []\n",
    "\n",
    "    x1 = transformer.transform(docs1).todense()\n",
    "    x2 = transformer.transform(docs2).todense()\n",
    "\n",
    "    x1 = cp.asarray(x1)\n",
    "    x2 = cp.asarray(x2)\n",
    "\n",
    "    x1 = scaler.transform(cp.asnumpy(x1))\n",
    "    x2 = scaler.transform(cp.asnumpy(x2))\n",
    "\n",
    "    return idxs, secondary_scaler.transform(np.abs(x1 - x2)), labels\n",
    "\n",
    "def vectorize_gpu(XX, Y, ordered_idxs, transformer, scaler, secondary_scaler, data_dict, batch_size=50000):\n",
    "    docs1 = []\n",
    "    docs2 = []\n",
    "    idxs = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "\n",
    "    num_entries = len(data_dict['text1'])\n",
    "\n",
    "    progress_bar = tqdm(total=num_entries, desc=\"Vectorizing dataset\", ncols=100)\n",
    "\n",
    "    for idx in range(num_entries):\n",
    "        docs1.append(data_dict['text1'][idx])\n",
    "        docs2.append(data_dict['text2'][idx])\n",
    "        labels.append(data_dict['score'][idx])\n",
    "        idxs.append(ordered_idxs[i])\n",
    "        i += 1\n",
    "\n",
    "        if len(labels) >= batch_size:\n",
    "            batch_data = (docs1, docs2, labels, idxs)\n",
    "            idxs, transformed_data, batch_labels = process_batch_gpu(batch_data, transformer, scaler, secondary_scaler)\n",
    "            if idxs:  # Only update if the batch was processed successfully\n",
    "                XX[idxs, :] = transformed_data\n",
    "                Y[idxs] = batch_labels\n",
    "\n",
    "            docs1 = []\n",
    "            docs2 = []\n",
    "            idxs = []\n",
    "            labels = []\n",
    "\n",
    "            progress_bar.update(len(idxs))  # Manually update progress\n",
    "\n",
    "    # Handle remaining data\n",
    "    if len(labels) > 0:\n",
    "        batch_data = (docs1, docs2, labels, idxs)\n",
    "        idxs, transformed_data, batch_labels = process_batch_gpu(batch_data, transformer, scaler, secondary_scaler)\n",
    "        if idxs:  # Only update if the batch was processed successfully\n",
    "            XX[idxs, :] = transformed_data\n",
    "            Y[idxs] = batch_labels\n",
    "            progress_bar.update(len(idxs))  # Manually update progress\n",
    "\n",
    "    progress_bar.close()\n",
    "    XX.flush()\n",
    "    Y.flush()\n",
    "    \n",
    "batch_size = 5000  # Increased batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorize_gpu(\n",
    "    XX_test, \n",
    "    Y_test, \n",
    "    test_idxs, \n",
    "    transformer, \n",
    "    scaler, \n",
    "    secondary_scaler, \n",
    "    test_dict,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:46:29<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:44:18<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:42:59<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:42:42<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:42:32<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:41:55<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                              | 0/30780 [1:42:25<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [58:27<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                                | 0/30780 [30:33<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [28:43<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                                | 0/30780 [30:02<?, ?it/s]\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [21:33<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [01:14<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n",
      "Skipping batch due to mismatched lengths of tokens and pos_tags.\n",
      "Processing batch with 5000 documents\n",
      "Skipping batch due to empty tokens.\n",
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [02:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n",
      "Skipping batch due to empty tokens.\n",
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [03:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n",
      "Skipping batch due to empty tokens.\n",
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [03:57<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [06:13<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [07:22<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [08:58<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [10:09<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [11:19<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [14:56<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [27:15<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [40:38<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                                | 0/97584 [53:21<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                              | 0/97584 [1:06:13<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   0%|                                              | 0/97584 [1:19:18<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 2584 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vectorizing dataset:   3%|▊                               | 2584/97584 [1:25:56<52:39:51,  2.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorize_gpu(\n",
    "    XX_train, \n",
    "    Y_train, \n",
    "    train_idxs, \n",
    "    transformer, \n",
    "    scaler, \n",
    "    secondary_scaler, \n",
    "    train_dict,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vectorize(\n",
    "#     XX_test, \n",
    "#     Y_test, \n",
    "#     test_idxs, \n",
    "#     transformer, \n",
    "#     scaler, \n",
    "#     secondary_scaler, \n",
    "#     test_dict,\n",
    "#     test_sz\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameters...\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END ........................alpha=0.0002069836309284027; total time=   0.0s\n",
      "[CV] END ........................alpha=0.0002069836309284027; total time=   0.0s\n",
      "[CV] END ........................alpha=0.0002069836309284027; total time=   0.1s\n",
      "[CV] END ........................alpha=0.0002069836309284027; total time=   0.0s\n",
      "[CV] END ........................alpha=0.0002069836309284027; total time=   0.0s\n",
      "[CV] END ...........................alpha=0.3257111378906439; total time=   0.1s\n",
      "[CV] END ...........................alpha=0.3257111378906439; total time=   0.0s\n",
      "[CV] END ...........................alpha=0.3257111378906439; total time=   0.0s\n",
      "[CV] END ...........................alpha=0.3257111378906439; total time=   0.1s\n",
      "[CV] END ...........................alpha=0.3257111378906439; total time=   0.1s\n",
      "Best params: {'alpha': 0.0002069836309284027}\n"
     ]
    }
   ],
   "source": [
    "print('Tuning parameters...', flush=True)\n",
    "\n",
    "\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "batch_size=100\n",
    "clf = SGDClassifier(loss='log_loss', alpha=0.01)\n",
    "n_iter_search = 2\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, verbose=2)\n",
    "for idxs in chunker(range(train_sz), batch_size):\n",
    "        random_search.fit(XX_train[idxs, :], Y_train[idxs])\n",
    "        break\n",
    "\n",
    "print('Best params:', random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da827435504410bbd3f2f130b9d6962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  0\n",
      "------------------------------\n",
      "AUC:  0.570031163380811\n",
      "Epoch -  1\n",
      "------------------------------\n",
      "AUC:  0.6144407247670979\n",
      "Epoch -  2\n",
      "------------------------------\n",
      "AUC:  0.600848206765084\n",
      "Epoch -  3\n",
      "------------------------------\n",
      "AUC:  0.6101171930580097\n",
      "Epoch -  4\n",
      "------------------------------\n",
      "AUC:  0.6255071343073737\n",
      "Epoch -  5\n",
      "------------------------------\n",
      "AUC:  0.6322024796441608\n",
      "Epoch -  6\n",
      "------------------------------\n",
      "AUC:  0.6444499542956785\n",
      "Epoch -  7\n",
      "------------------------------\n",
      "AUC:  0.6548354116661073\n",
      "Epoch -  8\n",
      "------------------------------\n",
      "AUC:  0.6569468246579565\n",
      "Epoch -  9\n",
      "------------------------------\n",
      "AUC:  0.627805852686422\n",
      "Epoch -  10\n",
      "------------------------------\n",
      "AUC:  0.6428407019677135\n",
      "Epoch -  11\n",
      "------------------------------\n",
      "AUC:  0.6209748003470572\n",
      "Epoch -  12\n",
      "------------------------------\n",
      "AUC:  0.6515784034942167\n",
      "Epoch -  13\n",
      "------------------------------\n",
      "AUC:  0.65471170600616\n",
      "Epoch -  14\n",
      "------------------------------\n",
      "AUC:  0.6451106720118557\n",
      "Epoch -  15\n",
      "------------------------------\n",
      "AUC:  0.6466005041197221\n",
      "Epoch -  16\n",
      "------------------------------\n",
      "AUC:  0.6506684990616365\n",
      "Epoch -  17\n",
      "------------------------------\n",
      "AUC:  0.6382178903391831\n",
      "Epoch -  18\n",
      "------------------------------\n",
      "AUC:  0.6567137832834069\n",
      "Epoch -  19\n",
      "------------------------------\n",
      "AUC:  0.6584731294477272\n",
      "Epoch -  20\n",
      "------------------------------\n",
      "AUC:  0.6648113414271147\n",
      "Epoch -  21\n",
      "------------------------------\n",
      "AUC:  0.6488788043833927\n",
      "Epoch -  22\n",
      "------------------------------\n",
      "AUC:  0.634295771347507\n",
      "Epoch -  23\n",
      "------------------------------\n",
      "AUC:  0.6593253345493004\n",
      "Epoch -  24\n",
      "------------------------------\n",
      "AUC:  0.6512319035163828\n",
      "Epoch -  25\n",
      "------------------------------\n",
      "AUC:  0.6725534613898758\n",
      "Epoch -  26\n",
      "------------------------------\n",
      "AUC:  0.6494964375961849\n",
      "Epoch -  27\n",
      "------------------------------\n",
      "AUC:  0.6563471121835266\n",
      "Epoch -  28\n",
      "------------------------------\n",
      "AUC:  0.6598182741288208\n",
      "Epoch -  29\n",
      "------------------------------\n",
      "AUC:  0.6361012166004851\n",
      "Epoch -  30\n",
      "------------------------------\n",
      "AUC:  0.6480252228745379\n",
      "Epoch -  31\n",
      "------------------------------\n",
      "AUC:  0.6647255735206261\n",
      "Epoch -  32\n",
      "------------------------------\n",
      "AUC:  0.6591603810030462\n",
      "Epoch -  33\n",
      "------------------------------\n",
      "AUC:  0.6274208576368439\n",
      "Epoch -  34\n",
      "------------------------------\n",
      "AUC:  0.6506383996520982\n",
      "Epoch -  35\n",
      "------------------------------\n",
      "AUC:  0.6571255528322958\n",
      "Epoch -  36\n",
      "------------------------------\n",
      "AUC:  0.6354435092474726\n",
      "Epoch -  37\n",
      "------------------------------\n",
      "AUC:  0.6594214846199152\n",
      "Epoch -  38\n",
      "------------------------------\n",
      "AUC:  0.6470531605646648\n",
      "Epoch -  39\n",
      "------------------------------\n",
      "AUC:  0.6521691798765877\n",
      "Epoch -  40\n",
      "------------------------------\n",
      "AUC:  0.653404708072882\n",
      "Epoch -  41\n",
      "------------------------------\n",
      "AUC:  0.6373641800560274\n",
      "Epoch -  42\n",
      "------------------------------\n",
      "AUC:  0.6492959275655112\n",
      "Epoch -  43\n",
      "------------------------------\n",
      "AUC:  0.6472397144165997\n",
      "Epoch -  44\n",
      "------------------------------\n",
      "AUC:  0.6510617609991196\n",
      "Epoch -  45\n",
      "------------------------------\n",
      "AUC:  0.6691893868865539\n",
      "Epoch -  46\n",
      "------------------------------\n",
      "AUC:  0.6497559705887941\n",
      "Epoch -  47\n",
      "------------------------------\n",
      "AUC:  0.6438467205829886\n",
      "Epoch -  48\n",
      "------------------------------\n",
      "AUC:  0.6615833539161109\n",
      "Epoch -  49\n",
      "------------------------------\n",
      "AUC:  0.6459743105825031\n"
     ]
    }
   ],
   "source": [
    "print('Training classifier...', flush=True)\n",
    "clf = SGDClassifier(loss='log_loss', alpha=random_search.best_params_['alpha'])\n",
    "batch_size=50000\n",
    "num_epochs = 50\n",
    "aucs = []\n",
    "for i in trange(num_epochs):\n",
    "    print('Epoch - ', i)\n",
    "    print('-' * 30)\n",
    "    for idxs in chunker(range(train_sz), batch_size):\n",
    "        clf.partial_fit(XX_train[idxs, :], Y_train[idxs], classes=[0, 1])\n",
    "\n",
    "    probs = clf.predict_proba(XX_test)[:, 1]\n",
    "    fpr, tpr, thresh = roc_curve(Y_test, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    print('AUC: ', roc_auc)\n",
    "    with open('/workspace/featurevector/temp_data/experiment_data.p', 'wb') as f:\n",
    "        pickle.dump((\n",
    "            aucs,\n",
    "            clf,\n",
    "            roc_auc,\n",
    "            transformer, \n",
    "            scaler,\n",
    "            secondary_scaler,\n",
    "            feature_sz,\n",
    "            train_sz,\n",
    "            train_idxs,\n",
    "            test_sz,\n",
    "            test_idxs\n",
    "        ), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure(go.Scatter(\n",
    "    x=np.arange(len(aucs)),\n",
    "    y=aucs\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('large_model.p', 'wb') as f:\n",
    "    pickle.dump((clf, transformer, scaler, secondary_scaler), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.646, 'c@1': 0.599, 'f_05_u': 0.606, 'F1': 0.653, 'brier': 0.627, 'overall': 0.626}\n"
     ]
    }
   ],
   "source": [
    "from pan20_verif_evaluator import evaluate_all\n",
    "results = evaluate_all(Y_test, probs)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loaded_test_datasets['pan14']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating darkreddit\n",
      "Evaluating imdb\n",
      "Evaluating pan11\n",
      "Evaluating pan13\n",
      "Evaluating pan14\n",
      "Evaluating pan15\n",
      "Evaluating pan20\n",
      "Evaluating reuters\n",
      "Evaluating victorian\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in loaded_test_datasets.items():\n",
    "    print(f\"Evaluating {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pan15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 200/200 [00:22<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.629, 'c@1': 0.565, 'f_05_u': 0.596, 'F1': 0.636, 'brier': 0.601, 'overall': 0.605}\n",
      "Evaluating pan20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries:  89%|████████▊ | 12148/13704 [1:39:28<11:18,  2.29it/s]  "
     ]
    }
   ],
   "source": [
    "from features import prepare_entry\n",
    "def vectorize_and_evaluate(dataset, transformer, scaler, secondary_scaler, clf):\n",
    "    probs = []\n",
    "    Y_test = []\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Processing entries\"):\n",
    "        text1 = example['text1']\n",
    "        text2 = example['text2']\n",
    "        score = example['same']\n",
    "\n",
    "        \n",
    "\n",
    "        preprocessed_doc1 = prepare_entry(text1, mode='fast', tokenizer='casual')\n",
    "        preprocessed_doc2 = prepare_entry(text2, mode='fast', tokenizer='casual')\n",
    "\n",
    "        if not preprocessed_doc1 or not preprocessed_doc2:\n",
    "            print(f\"Warning: parsing empty text for example with score {score}\")\n",
    "            continue\n",
    "\n",
    "        if len(preprocessed_doc1['tokens']) == 0 or len(preprocessed_doc2['tokens']) == 0:\n",
    "            print(f\"Warning: parsing empty text for example with score {score}\")\n",
    "            continue\n",
    "\n",
    "        try: \n",
    "            X1 = np.asarray(transformer.transform([preprocessed_doc1]).todense())\n",
    "            X2 = np.asarray(transformer.transform([preprocessed_doc2]).todense())\n",
    "        except:\n",
    "            print(preprocessed_doc1)\n",
    "            print(preprocessed_doc2)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        X1 = scaler.transform(X1)\n",
    "        X2 = scaler.transform(X2)\n",
    "        \n",
    "        X = secondary_scaler.transform(np.abs(X1 - X2))\n",
    "        \n",
    "        prob = clf.predict_proba(X)[0, 1]\n",
    "        probs.append(prob)\n",
    "        Y_test.append(score)\n",
    "\n",
    "    return Y_test, probs\n",
    "\n",
    "\n",
    "# Evaluate all datasets\n",
    "results_dict = {}\n",
    "for name, dataset in loaded_test_datasets.items():\n",
    "    print(f\"Evaluating {name}\")\n",
    "    Y_test, probs = vectorize_and_evaluate(dataset, transformer, scaler, secondary_scaler, clf)\n",
    "    results = evaluate_all(Y_test, probs)\n",
    "    results_dict[name] = results\n",
    "    print(results)\n",
    "\n",
    "# Print all results\n",
    "for dataset_name, results in results_dict.items():\n",
    "    print(f\"Results for {dataset_name}:\")\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
