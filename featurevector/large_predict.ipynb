{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7af406c-abcc-4fcb-9756-6b8e59d55982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from features import get_transformer, prepare_entry\n",
    "from tqdm import  tqdm\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import argparse\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dd77dc-e72c-4c02-a311-b1a68a328ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = 'large_model.p'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595766e4-8551-4a08-8a6e-c04e9f72762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(transformer, scaler, secondary_scaler, clf, ids, preprocessed_docs1, preprocessed_docs2, output_file):\n",
    "    print('Extracting features:', len(ids), file=sys.stderr)\n",
    "    X1 = scaler.transform(transformer.transform(preprocessed_docs1).todense())\n",
    "    X2 = scaler.transform(transformer.transform(preprocessed_docs2).todense())\n",
    "    X = secondary_scaler.transform(np.abs(X1 - X2))\n",
    "    print('Predicting...', file=sys.stderr)\n",
    "    probs = clf.predict_proba(X)[:, 1]\n",
    "    print('Writing to', output_file, file=sys.stderr)\n",
    "    with open(output_file, 'a') as f:\n",
    "        for i in range(len(ids)):\n",
    "            d = {\n",
    "                'id': ids[i],\n",
    "                'value': probs[i]\n",
    "            }\n",
    "            json.dump(d, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484196df-fe4d-4d10-ad66-b4e8f4da00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_entry(transformer, scaler, secondary_scaler, clf, idx, preprocessed_doc1, preprocessed_doc2, f_output_file):    \n",
    "    try:\n",
    "        X1 = np.asarray(transformer.transform([preprocessed_doc1]).todense())\n",
    "        X2 = np.asarray(transformer.transform([preprocessed_doc2]).todense())\n",
    "        \n",
    "        # Scale the data\n",
    "        X1 = scaler.transform(X1)\n",
    "        X2 = scaler.transform(X2)\n",
    "        \n",
    "        # Calculate the absolute difference and apply secondary scaling\n",
    "        X = secondary_scaler.transform(np.abs(X1 - X2))\n",
    "        \n",
    "        # Predict the probability\n",
    "        prob = clf.predict_proba(X)[0, 1]\n",
    "    except Exception as e:\n",
    "        print('Exception predicting:', e)\n",
    "        prob = 0.5\n",
    "    d = {\n",
    "        'id': idx,\n",
    "        'value': prob\n",
    "    }\n",
    "    print(prob)\n",
    "    json.dump(d, f_output_file)\n",
    "    f_output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f65df358-3732-4ffc-95e0-a9c8b77f8664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing answers to: answers2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 54.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999999999999989\n",
      "1.0\n",
      "3.9468433133058235e-17\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution complete\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='Prediction Script: PAN 2021')\n",
    "# parser.add_argument('-i', type=str,\n",
    "#                     help='Evaluaiton dir')\n",
    "# parser.add_argument('-o', type=str, \n",
    "#                     help='Output dir')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # validate:\n",
    "# if not args.i:\n",
    "#     raise ValueError('Eval dir path is required')\n",
    "# if not args.o:\n",
    "#     raise ValueError('Output dir path is required')\n",
    "    \n",
    "    \n",
    "# input_file = os.path.join(args.i, 'pairs.jsonl')\n",
    "# output_file = os.path.join(args.o, 'answers.jsonl')\n",
    "\n",
    "input_file = 'input.jsonl'\n",
    "output_file = 'answers2.jsonl'\n",
    "print(\"Writing answers to:\", output_file , file=sys.stdout, flush=True)\n",
    "\n",
    "\n",
    "with open(MODEL_FILE, 'rb') as f:\n",
    "    clf, transformer, scaler, secondary_scaler = pickle.load(f)\n",
    "\n",
    "with open(input_file, 'r') as f, open(output_file, 'w') as f_output_file:\n",
    "    i = 0\n",
    "    for l in tqdm(f):\n",
    "        if i % 100 == 0:\n",
    "            print(i, flush=True)\n",
    "        i += 1\n",
    "        d = json.loads(l)\n",
    "        idx = d['id']\n",
    "        preprocessed_doc1 = prepare_entry(d['text1'], mode='accurate', tokenizer='casual')\n",
    "        preprocessed_doc2 = prepare_entry(d['text2'], mode='accurate', tokenizer='casual')\n",
    "        process_single_entry(transformer, scaler, secondary_scaler, clf, idx, preprocessed_doc1, preprocessed_doc2, f_output_file) \n",
    "        \n",
    "print(\"Execution complete\", file=sys.stderr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19fde3ad-87d9-4367-a265-22264c7bb563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 5it [00:00, 15.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define your custom transformers and get_transformer function here\n",
    "# ...\n",
    "\n",
    "def process_single_entry(transformer, scaler, secondary_scaler, clf, idx, preprocessed_doc1, preprocessed_doc2, f_output_file):\n",
    "    try:\n",
    "        X1 = np.asarray(transformer.transform([preprocessed_doc1]).todense())\n",
    "        X2 = np.asarray(transformer.transform([preprocessed_doc2]).todense())\n",
    "        \n",
    "        X1 = scaler.transform(X1)\n",
    "        X2 = scaler.transform(X2)\n",
    "        \n",
    "        X = secondary_scaler.transform(np.abs(X1 - X2))\n",
    "        \n",
    "        prob = clf.predict_proba(X)[0, 1]\n",
    "        explanation = clf.coef_.flatten().tolist()  # Get the coefficients as the feature importances\n",
    "    except Exception as e:\n",
    "        print('Exception predicting:', e)\n",
    "        prob = 0.5\n",
    "        explanation = None\n",
    "\n",
    "    d = {\n",
    "        'id': idx,\n",
    "        'value': prob,\n",
    "        'explanation': explanation\n",
    "    }\n",
    "    \n",
    "    json.dump(d, f_output_file)\n",
    "    f_output_file.write('\\n')\n",
    "\n",
    "def process_file(file_path, transformer, scaler, secondary_scaler, clf, output_file_path):\n",
    "    with open(file_path, 'r') as f, open(output_file_path, 'w') as f_output_file:\n",
    "        for i, l in enumerate(tqdm(f, desc=\"Processing entries\")):\n",
    "            if i % 100 == 0:\n",
    "                print(i, flush=True)\n",
    "\n",
    "            d = json.loads(l)\n",
    "            idx = d['id']\n",
    "            preprocessed_doc1 = prepare_entry(d['text1'], mode='accurate', tokenizer='casual')\n",
    "            preprocessed_doc2 = prepare_entry(d['text2'], mode='accurate', tokenizer='casual')\n",
    "            process_single_entry(transformer, scaler, secondary_scaler, clf, idx, preprocessed_doc1, preprocessed_doc2, f_output_file)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'input.jsonl'\n",
    "output_file_path = 'answers1.jsonl'\n",
    "\n",
    "# Call process_file with appropriate arguments\n",
    "process_file(file_path, transformer, scaler, secondary_scaler, clf, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ba875ba-f640-4fee-9171-156ed8851afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# def get_all_feature_names(feature_union):\n",
    "#     feature_names = []\n",
    "#     for name, transformer in feature_union.transformer_list:\n",
    "#         if hasattr(transformer, 'get_feature_names_out'):\n",
    "#             feature_names.extend(transformer.get_feature_names_out())\n",
    "#         else:\n",
    "#             feature_names.append(name)\n",
    "#     return np.array(feature_names)\n",
    "\n",
    "# # Assuming your FeatureUnion is called `transformer`\n",
    "# all_feature_names = get_all_feature_names(transformer)\n",
    "\n",
    "# # Calculate feature importances and map them to feature names\n",
    "# feature_importances = clf.coef_.flatten()\n",
    "# importance_dict = dict(zip(all_feature_names, feature_importances))\n",
    "\n",
    "# # Printing feature importances\n",
    "# for feature, importance in importance_dict.items():\n",
    "#     print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aadf7ea-a51b-4dc9-868c-e0f6b0a5615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: specifying, Importance: 5.5193610191345215\n",
      "Feature: .., Importance: -2.052586317062378\n",
      "Feature: entropy, Importance: 1.7252956628799438\n",
      "Feature: viz, Importance: -1.7221406698226929\n",
      "Feature: whereafter, Importance: -1.5986037254333496\n",
      "Feature: : NN, Importance: -1.3982906341552734\n",
      "Feature: just, Importance: -1.3504716157913208\n",
      "Feature:  i', Importance: -1.3381776809692383\n",
      "Feature: nv, Importance: -1.2608269453048706\n",
      "Feature: o,, Importance: 1.256258487701416\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# def get_all_feature_names(feature_union):\n",
    "#     feature_names = []\n",
    "#     for name, transformer in feature_union.transformer_list:\n",
    "#         if hasattr(transformer, 'get_feature_names_out'):\n",
    "#             feature_names.extend(transformer.get_feature_names_out())\n",
    "#         else:\n",
    "#             feature_names.append(name)\n",
    "#     return np.array(feature_names)\n",
    "\n",
    "# # Assuming your FeatureUnion is called `transformer`\n",
    "# all_feature_names = get_all_feature_names(transformer)\n",
    "\n",
    "# # Calculate feature importances and map them to feature names\n",
    "# feature_importances = clf.coef_.flatten()\n",
    "# importance_dict = dict(zip(all_feature_names, feature_importances))\n",
    "\n",
    "# # Sorting the features by their importance\n",
    "# sorted_features = sorted(importance_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "\n",
    "# # Getting the top 10 most important features\n",
    "# top_10_features = sorted_features[:10]\n",
    "\n",
    "# # Printing the top 10 most important features\n",
    "# for feature, importance in top_10_features:\n",
    "#     print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41cc2692-65ec-46e9-83ec-318f06cbef8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features (Absolute Importance):\n",
      "Feature: specifying, Importance: 5.5193610191345215\n",
      "Feature: .., Importance: -2.052586317062378\n",
      "Feature: entropy, Importance: 1.7252956628799438\n",
      "Feature: viz, Importance: -1.7221406698226929\n",
      "Feature: whereafter, Importance: -1.5986037254333496\n",
      "Feature: : NN, Importance: -1.3982906341552734\n",
      "Feature: just, Importance: -1.3504716157913208\n",
      "Feature:  i', Importance: -1.3381776809692383\n",
      "Feature: nv, Importance: -1.2608269453048706\n",
      "Feature: o,, Importance: 1.256258487701416\n",
      "\n",
      "Top 5 Positive Features:\n",
      "Feature: specifying, Importance: 5.5193610191345215\n",
      "Feature: entropy, Importance: 1.7252956628799438\n",
      "Feature: o,, Importance: 1.256258487701416\n",
      "Feature: squ, Importance: 1.1695750951766968\n",
      "Feature: e., Importance: 1.1690033674240112\n",
      "\n",
      "Top 5 Negative Features:\n",
      "Feature: .., Importance: -2.052586317062378\n",
      "Feature: viz, Importance: -1.7221406698226929\n",
      "Feature: whereafter, Importance: -1.5986037254333496\n",
      "Feature: : NN, Importance: -1.3982906341552734\n",
      "Feature: just, Importance: -1.3504716157913208\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "def get_all_feature_names(feature_union):\n",
    "    feature_names = []\n",
    "    for name, transformer in feature_union.transformer_list:\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            feature_names.extend(transformer.get_feature_names_out())\n",
    "        else:\n",
    "            feature_names.append(name)\n",
    "    return np.array(feature_names)\n",
    "\n",
    "# Assuming your FeatureUnion is called `transformer`\n",
    "all_feature_names = get_all_feature_names(transformer)\n",
    "\n",
    "# Calculate feature importances and map them to feature names\n",
    "feature_importances = clf.coef_.flatten()\n",
    "importance_dict = dict(zip(all_feature_names, feature_importances))\n",
    "\n",
    "# Sorting the features by their absolute importance\n",
    "sorted_features = sorted(importance_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "\n",
    "# Getting the top 10 most important features\n",
    "top_10_features = sorted_features[:10]\n",
    "\n",
    "print(\"Top 10 Features (Absolute Importance):\")\n",
    "for feature, importance in top_10_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "# Separating positive and negative importances\n",
    "positive_importances = [(feature, importance) for feature, importance in sorted_features if importance > 0]\n",
    "negative_importances = [(feature, importance) for feature, importance in sorted_features if importance < 0]\n",
    "\n",
    "print(\"\\nTop 5 Positive Features:\")\n",
    "for feature, importance in positive_importances[:5]:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "print(\"\\nTop 5 Negative Features:\")\n",
    "for feature, importance in negative_importances[:5]:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6c0140e-7396-4503-9a3b-79f3ef818134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 5it [00:00, 45.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_top_features(differences, coef, feature_names, top_n=10):\n",
    "    # Calculate the importance of each feature by multiplying differences with the coefficients\n",
    "    importances = np.abs(differences * coef)\n",
    "    top_indices = np.argsort(importances)[-top_n:][::-1]\n",
    "    top_features = [(feature_names[i], float(importances[i])) for i in top_indices]  # Convert to native float\n",
    "    return top_features\n",
    "\n",
    "def process_single_entry(transformer, scaler, secondary_scaler, clf, idx, preprocessed_doc1, preprocessed_doc2, f_output_file, all_feature_names):    \n",
    "    try:\n",
    "        X1 = scaler.transform(np.asarray(transformer.transform([preprocessed_doc1]).todense()))\n",
    "        X2 = scaler.transform(np.asarray(transformer.transform([preprocessed_doc2]).todense()))\n",
    "        differences = np.abs(X1 - X2)\n",
    "        X = secondary_scaler.transform(differences)\n",
    "        prob = clf.predict_proba(X)[0, 1]\n",
    "\n",
    "        # Extract top features for the current comparison\n",
    "        top_features = get_top_features(differences.flatten(), clf.coef_.flatten(), all_feature_names, top_n=10)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Exception predicting:', e)\n",
    "        prob = 0.5\n",
    "        top_features = []\n",
    "\n",
    "    d = {\n",
    "        'id': idx,\n",
    "        'value': float(prob),  # Convert to native float\n",
    "        'top_features': [{'feature': feature, 'importance': importance} for feature, importance in top_features]\n",
    "    }\n",
    "    \n",
    "    json.dump(d, f_output_file)\n",
    "    f_output_file.write('\\n')\n",
    "\n",
    "def main(input_file, output_file, transformer, scaler, secondary_scaler, clf):\n",
    "    all_feature_names = get_all_feature_names(transformer)\n",
    "\n",
    "    with open(input_file, 'r') as f_input_file, open(output_file, 'w') as f_output_file:\n",
    "        for i, line in enumerate(tqdm(f_input_file, desc=\"Processing entries\")):\n",
    "            d = json.loads(line)\n",
    "            idx = d['id']\n",
    "            preprocessed_doc1 = prepare_entry(d['text1'], mode='accurate', tokenizer='casual')\n",
    "            preprocessed_doc2 = prepare_entry(d['text2'], mode='accurate', tokenizer='casual')\n",
    "            process_single_entry(transformer, scaler, secondary_scaler, clf, idx, preprocessed_doc1, preprocessed_doc2, f_output_file, all_feature_names)\n",
    "\n",
    "def get_all_feature_names(feature_union):\n",
    "    feature_names = []\n",
    "    for name, transformer in feature_union.transformer_list:\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            feature_names.extend(transformer.get_feature_names_out())\n",
    "        else:\n",
    "            feature_names.append(name)\n",
    "    return np.array(feature_names)\n",
    "\n",
    "# Example usage\n",
    "input_file = 'input.jsonl'\n",
    "output_file = 'output.jsonl'\n",
    "main(input_file, output_file, transformer, scaler, secondary_scaler, clf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
